/**
 * visual-test test - è¿è¡Œè§†è§‰å›å½’æµ‹è¯•
 *
 * åŠ è½½é…ç½®ï¼Œè°ƒç”¨ Orchestrator æ‰§è¡Œå®Œæ•´æµ‹è¯•æµç¨‹ï¼Œè¾“å‡ºç»“æœã€‚
 */

import chalk from 'chalk';
import type { Command } from 'commander';
import { loadConfig, loadConfigFromFile } from '../../core/config/loader';
import { VisualTestOrchestrator } from '../../core/orchestrator';
import { createSpinner } from '../ui/spinner';
import {
  formatSummary,
  formatFailures,
  formatReportPaths,
  formatLLMCost,
} from '../ui/formatter';
import { logger, LogLevel, parseLogLevel } from '../../utils/logger';

/**
 * æ³¨å†Œ test å‘½ä»¤
 */
export function registerTestCommand(program: Command): void {
  program
    .command('test [targets...]')
    .description('Run visual regression tests')
    .option('-c, --config <path>', 'Config file path')
    .option('--debug', 'Enable debug logging')
    .option('--ci', 'CI mode: fail on diff')
    .option('--update', 'Update baselines for failed tests')
    .option('--no-llm', 'Disable LLM analysis')
    .action(
      async (
        targets: string[],
        options: {
          config?: string;
          debug?: boolean;
          ci?: boolean;
          update?: boolean;
          llm?: boolean;
        },
      ) => {
        await runTest(targets, options);
      },
    );
}

async function runTest(
  targetNames: string[],
  options: {
    config?: string;
    debug?: boolean;
    ci?: boolean;
    update?: boolean;
    llm?: boolean;
  },
): Promise<void> {
  // åŠ è½½é…ç½®
  const config = options.config
    ? await loadConfigFromFile(options.config)
    : await loadConfig();

  // åº”ç”¨é…ç½®ä¸­çš„æ—¥å¿—çº§åˆ«ï¼ˆ--debug è¦†ç›–é…ç½®ï¼‰
  if (options.debug) {
    logger.setLevel(LogLevel.DEBUG);
  } else if (config.logging.level) {
    logger.setLevel(parseLogLevel(config.logging.level));
  }

  // --no-llm è¦†ç›–ï¼šåˆ›å»ºæ–°çš„ llm é…ç½®å¯¹è±¡ï¼Œé¿å…ä¿®æ”¹ Zod è¾“å‡º
  const llmConfig =
    options.llm === false ? { ...config.llm, enabled: false } : config.llm;

  // ç›®æ ‡è¿‡æ»¤æç¤º
  if (targetNames.length > 0) {
    console.log(chalk.gray(`Running tests for: ${targetNames.join(', ')}`));
  } else {
    console.log(chalk.gray('Running all visual tests...'));
  }

  // è¿è¡Œæµ‹è¯•
  const spinner = createSpinner('Running visual tests...').start();
  const effectiveConfig =
    llmConfig !== config.llm ? { ...config, llm: llmConfig } : config;
  const orchestrator = new VisualTestOrchestrator(effectiveConfig);
  const startTime = Date.now();

  let results;
  try {
    results = await orchestrator.runTests(
      targetNames.length > 0 ? targetNames : undefined,
    );
    spinner.succeed('Visual tests completed');
  } catch (error) {
    spinner.fail(error instanceof Error ? error.message : 'Test failed');
    console.error(
      chalk.red(error instanceof Error ? error.message : String(error)),
    );
    process.exitCode = 1;
    return;
  }

  const elapsed = Date.now() - startTime;

  // é¦–æ¬¡è¿è¡Œå¼•å¯¼ï¼šæ£€æµ‹æ˜¯å¦å¤§é‡æµ‹è¯•å› ç¼ºå°‘åŸºå‡†å›¾è€Œå¤±è´¥
  const failedResults = results.filter((r) => !r.passed);
  const missingBaselineResults = failedResults.filter(
    (r) => r.mismatchPercentage === 100,
  );
  if (
    !options.update &&
    missingBaselineResults.length > 0 &&
    missingBaselineResults.length === failedResults.length
  ) {
    console.log('');
    console.log(
      chalk.yellow(
        'Hint: All failures appear to be missing baselines (first run?).',
      ),
    );
    console.log(
      chalk.yellow(
        `  Run ${chalk.cyan('visual-test test --update')} to capture initial baselines.`,
      ),
    );
  }

  // å¦‚æœå¯ç”¨äº† --updateï¼Œæ›´æ–°å¤±è´¥ç”¨ä¾‹çš„åŸºå‡†å›¾
  if (options.update) {
    if (failedResults.length > 0) {
      console.log(
        chalk.yellow(`\nğŸ“ Updating ${failedResults.length} baseline(s)...`),
      );
      await orchestrator.updateBaselines(failedResults);
      console.log(chalk.green('âœ“ Baselines updated successfully\n'));
    } else {
      console.log(chalk.gray('No failed tests to update\n'));
    }
  }

  // è¾“å‡ºç»“æœ
  console.log(formatSummary(results, elapsed));

  // è¾“å‡ºå¤±è´¥è¯¦æƒ…
  const failureOutput = formatFailures(results);
  if (failureOutput) {
    console.log(failureOutput);
  }

  // è¾“å‡º LLM æˆæœ¬æ‘˜è¦
  const llmStats = orchestrator.getLLMStats();
  const costOutput = formatLLMCost(llmStats);
  if (costOutput) {
    console.log(costOutput);
  }

  // è¾“å‡ºæŠ¥å‘Šè·¯å¾„
  console.log(
    formatReportPaths(
      config.directories.reports,
      config.report.formats,
      config.report.conclusion,
    ),
  );

  // CI æ¨¡å¼ä¸‹æŒ‰ severity é˜ˆå€¼åˆ¤æ–­æ˜¯å¦é€€å‡ºéé›¶
  const hasFailed = results.some((r) => !r.passed);
  if (hasFailed && (options.ci || config.ci.failOnDiff)) {
    const severityOrder: Record<string, number> = {
      critical: 0,
      major: 1,
      minor: 2,
      trivial: 3,
    };
    const thresholdLevel = severityOrder[config.ci.failOnSeverity] ?? 1;

    const hasSignificantIssues = results.some((r) => {
      if (r.passed) return false;
      // æ—  LLM åˆ†ææ—¶ï¼Œè§†ä¸ºæ˜¾è‘—é—®é¢˜
      if (!r.analysis) return true;
      return r.analysis.differences.some(
        (d) => (severityOrder[d.severity] ?? 0) <= thresholdLevel,
      );
    });

    if (hasSignificantIssues) {
      process.exitCode = 1;
    }
  }
}
