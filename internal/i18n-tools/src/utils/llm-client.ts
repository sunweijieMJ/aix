import OpenAI from 'openai';
import type { LLMConfig, LocaleConfig, PromptsConfig } from '../config';
import {
  DEFAULT_LLM_MAX_RETRIES,
  DEFAULT_LLM_TEMPERATURE,
  DEFAULT_LLM_TIMEOUT,
} from '../config';
import { ConcurrencyController } from './concurrency-controller';
import { LoggerUtils } from './logger';
import {
  getIdGenerationSystemPrompt,
  getIdGenerationUserPrompt,
  getTranslationSystemPrompt,
  getTranslationUserPrompt,
} from './prompts';
import type { Translations } from './types';

/**
 * LLM å®¢æˆ·ç«¯
 */
export class LLMClient {
  private openai: OpenAI;
  private model: string;
  private temperature: number;
  private concurrencyController: ConcurrencyController;
  private locale?: LocaleConfig;
  private prompts?: PromptsConfig;

  constructor(
    config: LLMConfig,
    maxConcurrency: number = 5,
    locale?: LocaleConfig,
    prompts?: PromptsConfig,
  ) {
    this.openai = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseURL,
      timeout: config.timeout ?? DEFAULT_LLM_TIMEOUT,
      maxRetries: config.maxRetries ?? DEFAULT_LLM_MAX_RETRIES,
    });
    this.model = config.model;
    this.temperature = config.temperature ?? DEFAULT_LLM_TEMPERATURE;
    this.concurrencyController = new ConcurrencyController(maxConcurrency);
    this.locale = locale;
    this.prompts = prompts;
  }

  /**
   * è°ƒç”¨ LLM chat completion
   */
  private async chatCompletion(
    systemPrompt: string,
    userPrompt: string,
  ): Promise<string> {
    const response = await this.openai.chat.completions.create({
      model: this.model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      temperature: this.temperature,
      response_format: { type: 'json_object' },
    });

    const content = response.choices[0]?.message?.content;
    if (!content) {
      throw new Error('LLM è¿”å›å†…å®¹ä¸ºç©º');
    }
    return content;
  }

  /**
   * æ¸…ç† LLM è¿”å›çš„ JSON æ–‡æœ¬ï¼ˆå»é™¤ markdown code fence ç­‰ï¼‰
   */
  private cleanJsonResponse(text: string): string {
    let cleaned = text.trim();
    // å»é™¤ markdown code fence
    if (cleaned.startsWith('```')) {
      cleaned = cleaned
        .replace(/^```(?:json)?\s*\n?/, '')
        .replace(/\n?```\s*$/, '');
    }
    return cleaned.trim();
  }

  /**
   * ç”Ÿæˆè¯­ä¹‰IDåˆ—è¡¨ï¼ˆæ”¯æŒå¹¶å‘åˆ†æ‰¹ï¼‰
   */
  async generateSemanticIds(
    textList: string[],
    batchSize: number = 10,
  ): Promise<string[]> {
    if (textList.length === 0) return [];

    if (textList.length <= batchSize) {
      return this.generateSemanticIdsBatch(textList);
    }

    const batches: string[][] = [];
    const totalBatches = Math.ceil(textList.length / batchSize);

    for (let i = 0; i < totalBatches; i++) {
      const startIndex = i * batchSize;
      const endIndex = Math.min(startIndex + batchSize, textList.length);
      batches.push(textList.slice(startIndex, endIndex));
    }

    LoggerUtils.info(
      `ğŸ“Š éœ€è¦åˆ† ${totalBatches} æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ ${batchSize} ä¸ªæ–‡æœ¬`,
    );
    LoggerUtils.info(
      `ğŸ”„ ä½¿ç”¨å¹¶å‘å¤„ç†ï¼Œæœ€å¤§å¹¶å‘æ•°: ${this.concurrencyController.getStatus().maxConcurrency}`,
    );

    const batchPromises = batches.map((batch, index) =>
      this.concurrencyController.add(async () => {
        LoggerUtils.info(
          `ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ ${index + 1}/${totalBatches} æ‰¹æ¬¡ (${batch.length} ä¸ªæ–‡æœ¬)...`,
        );

        try {
          const results = await this.generateSemanticIdsBatch(batch);
          LoggerUtils.success(`âœ… ç¬¬ ${index + 1} æ‰¹æ¬¡å¤„ç†å®Œæˆ`);
          return results;
        } catch (error) {
          LoggerUtils.error(`âŒ ç¬¬ ${index + 1} æ‰¹æ¬¡å¤„ç†å¤±è´¥:`, error);
          throw error;
        }
      }),
    );

    const settledResults = await Promise.allSettled(batchPromises);

    const failedBatches = settledResults.filter((r) => r.status === 'rejected');
    if (failedBatches.length > 0) {
      throw new Error(`${failedBatches.length}/${totalBatches} ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥`);
    }

    const results = settledResults
      .filter(
        (r): r is PromiseFulfilledResult<string[]> => r.status === 'fulfilled',
      )
      .map((r) => r.value)
      .flat();

    LoggerUtils.success(
      `ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ ${results.length} ä¸ªè¯­ä¹‰ID`,
    );
    return results;
  }

  /**
   * å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è¯­ä¹‰IDç”Ÿæˆ
   */
  private async generateSemanticIdsBatch(
    textList: string[],
  ): Promise<string[]> {
    const rawContent = await this.chatCompletion(
      getIdGenerationSystemPrompt(this.locale, this.prompts),
      getIdGenerationUserPrompt(textList, this.locale, this.prompts),
    );

    try {
      const cleaned = this.cleanJsonResponse(rawContent);
      const parsed = JSON.parse(cleaned);

      if (!parsed.id_list || !Array.isArray(parsed.id_list)) {
        throw new Error('LLM è¿”å›æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘ id_list æ•°ç»„');
      }

      return parsed.id_list as string[];
    } catch (error) {
      if (error instanceof SyntaxError) {
        throw new Error(
          `LLM è¿”å›çš„ JSON è§£æå¤±è´¥: ${rawContent.slice(0, 200)}`,
          { cause: error },
        );
      }
      throw error;
    }
  }

  /**
   * æ‰¹é‡ç¿»è¯‘ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
   */
  async batchTranslate(
    batches: Translations[],
    onProgress?: (current: number, total: number) => void,
  ): Promise<Translations[]> {
    if (batches.length === 0) return [];

    LoggerUtils.info(`ğŸ”„ å¼€å§‹æ‰¹é‡ç¿»è¯‘ï¼Œå…± ${batches.length} æ‰¹æ¬¡`);
    LoggerUtils.info(
      `ğŸ”„ ä½¿ç”¨å¹¶å‘å¤„ç†ï¼Œæœ€å¤§å¹¶å‘æ•°: ${this.concurrencyController.getStatus().maxConcurrency}`,
    );

    const results: Translations[] = new Array(batches.length);
    let successCount = 0;
    let failedCount = 0;

    const failedIndices: number[] = [];

    const batchPromises = batches.map((batch, index) =>
      this.concurrencyController.add(async () => {
        const jsonText = JSON.stringify(batch, null, 2);

        try {
          const translatedJsonText = await this.translateJson(jsonText);
          const translatedBatch: Translations = JSON.parse(translatedJsonText);
          results[index] = translatedBatch;
          successCount++;

          if (onProgress) {
            onProgress(successCount + failedCount, batches.length);
          }

          LoggerUtils.success(
            `âœ… ç¿»è¯‘æ‰¹æ¬¡ ${index + 1}/${batches.length} å®Œæˆ`,
          );
        } catch (error) {
          LoggerUtils.error(`âŒ ç¿»è¯‘æ‰¹æ¬¡ ${index + 1} å¤±è´¥:`, error);
          failedCount++;
          failedIndices.push(index);

          if (onProgress) {
            onProgress(successCount + failedCount, batches.length);
          }
        }
      }),
    );

    await Promise.all(batchPromises);

    // ä»ç»“æœä¸­ç§»é™¤å¤±è´¥çš„æ‰¹æ¬¡ï¼ˆä¿ç•™ä¸º undefinedï¼‰ï¼Œè°ƒç”¨æ–¹éœ€è¦å¤„ç†
    // ç”¨åŸå§‹æ•°æ®å¡«å……å¤±è´¥æ‰¹æ¬¡ï¼Œä½†æ ‡è®° en-US ä¸ºç©ºä»¥é¿å…è¯¯å†™å…¥
    for (const idx of failedIndices) {
      results[idx] = batches[idx]!;
    }

    if (failedCount > 0) {
      LoggerUtils.warn(
        `âš ï¸ æ‰¹é‡ç¿»è¯‘å®Œæˆ: ${successCount} ä¸ªæ‰¹æ¬¡æˆåŠŸ, ${failedCount} ä¸ªæ‰¹æ¬¡å¤±è´¥`,
      );
    } else {
      LoggerUtils.success(`ğŸ‰ æ‰¹é‡ç¿»è¯‘å®Œæˆï¼Œå…¨éƒ¨ ${successCount} ä¸ªæ‰¹æ¬¡æˆåŠŸ`);
    }
    return results;
  }

  /**
   * ç¿»è¯‘JSONæ–‡æœ¬
   */
  async translateJson(jsonText: string): Promise<string> {
    const rawContent = await this.chatCompletion(
      getTranslationSystemPrompt(this.locale, this.prompts),
      getTranslationUserPrompt(jsonText, this.locale, this.prompts),
    );

    try {
      const cleaned = this.cleanJsonResponse(rawContent);
      // éªŒè¯è¿”å›çš„æ˜¯æœ‰æ•ˆ JSON
      JSON.parse(cleaned);
      return cleaned;
    } catch (error) {
      if (error instanceof SyntaxError) {
        throw new Error(
          `LLM ç¿»è¯‘è¿”å›çš„ JSON è§£æå¤±è´¥: ${rawContent.slice(0, 200)}`,
          { cause: error },
        );
      }
      throw error;
    }
  }

  /**
   * è·å–å¹¶å‘æ§åˆ¶å™¨çŠ¶æ€
   */
  getConcurrencyStatus(): {
    running: number;
    queued: number;
    maxConcurrency: number;
  } {
    return this.concurrencyController.getStatus();
  }

  /**
   * è°ƒæ•´å¹¶å‘æ•°
   */
  adjustConcurrency(newMaxConcurrency: number): void {
    if (newMaxConcurrency < 1) {
      LoggerUtils.warn(`âš ï¸ æ— æ•ˆçš„å¹¶å‘æ•°: ${newMaxConcurrency}ï¼Œå¿…é¡»å¤§äº0`);
      return;
    }
    LoggerUtils.info(
      `ğŸ”§ è°ƒæ•´æœ€å¤§å¹¶å‘æ•°: ${this.concurrencyController.getStatus().maxConcurrency} -> ${newMaxConcurrency}`,
    );
    this.concurrencyController = new ConcurrencyController(newMaxConcurrency);
  }

  /**
   * ä¸ºæ–‡ä»¶ç”Ÿæˆè¯­ä¹‰ID
   */
  async generateSemanticIdsForFiles(
    fileGroups: Record<string, string[]>,
    skipLLM: boolean = false,
  ): Promise<Record<string, string[]>> {
    const results: Record<string, string[]> = {};

    if (skipLLM) {
      LoggerUtils.info('ğŸ”„ æ£€æµ‹åˆ° --skip-llmï¼Œå°†ç”±è°ƒç”¨æ–¹ä½¿ç”¨æœ¬åœ°IDç”Ÿæˆç­–ç•¥...');
      for (const filePath of Object.keys(fileGroups)) {
        results[filePath] = [];
      }
      return results;
    }

    LoggerUtils.info(
      `ğŸš€ å¼€å§‹é€šè¿‡LLMä¸º ${Object.keys(fileGroups).length} ä¸ªæ–‡ä»¶ç”Ÿæˆè¯­ä¹‰ID...`,
    );

    const promises = Object.entries(fileGroups).map(([filePath, texts]) =>
      this.concurrencyController.add(async () => {
        LoggerUtils.info(
          `ğŸ”„ æ­£åœ¨å¤„ç†æ–‡ä»¶: ${filePath} (${texts.length} ä¸ªæ–‡æœ¬)...`,
        );

        try {
          const allIds = await this.generateSemanticIds(texts);
          results[filePath] = allIds;
          LoggerUtils.success(
            `âœ… æ–‡ä»¶ ${filePath} çš„è¯­ä¹‰IDç”Ÿæˆå®Œæˆï¼Œå…± ${allIds.length} ä¸ªID`,
          );
        } catch {
          LoggerUtils.warn(
            `âš ï¸ æ–‡ä»¶ ${filePath} çš„LLM APIè°ƒç”¨å¤±è´¥ï¼Œå°†ç”±è°ƒç”¨æ–¹ä½¿ç”¨æœ¬åœ°IDç”Ÿæˆå…œåº•`,
          );
          results[filePath] = [];
        }
      }),
    );

    await Promise.all(promises);
    LoggerUtils.success(
      `ğŸ‰ æ‰€æœ‰æ–‡ä»¶çš„è¯­ä¹‰IDç”Ÿæˆå®Œæˆï¼Œå…± ${Object.values(results).flat().length} ä¸ªID`,
    );
    return results;
  }
}
